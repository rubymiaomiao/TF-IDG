{
    "sourceFile": "run_inference.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 8,
            "patches": [
                {
                    "date": 1754658558284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1754658577045,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,14 +14,12 @@\n from omegaconf import OmegaConf\n from PIL import Image\n import glob\n from atten.shareatten import *\n-from scipy.ndimage import rotate, zoom, binary_dilation, binary_erosion\n-from memory_profiler import profile\n-from skimage.morphology import disk\n \n \n \n+\n save_memory = False #save memory\n \n config = OmegaConf.load('./configs/inference.yaml')\n model_ckpt =  config.pretrained_model\n"
                },
                {
                    "date": 1754658589331,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,12 +14,12 @@\n from omegaconf import OmegaConf\n from PIL import Image\n import glob\n from atten.shareatten import *\n+from memory_profiler import profile\n \n \n \n-\n save_memory = False #save memory\n \n config = OmegaConf.load('./configs/inference.yaml')\n model_ckpt =  config.pretrained_model\n"
                },
                {
                    "date": 1754658596567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,8 @@\n from omegaconf import OmegaConf\n from PIL import Image\n import glob\n from atten.shareatten import *\n-from memory_profiler import profile\n \n \n \n save_memory = False #save memory\n"
                },
                {
                    "date": 1754658603852,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -323,12 +323,8 @@\n         torch.cuda.ipc_collect()\n \n     return raw_background, item\n \n-\n-\n-\n-\n def select_limited_images(ref_image_paths, fraction=1/3):\n \n     num_images_to_select = max(1, int(len(ref_image_paths) * fraction))\n     \n"
                },
                {
                    "date": 1754658633411,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -374,12 +374,14 @@\n                     tar_mask = Image.open(tar_mask_path).convert('L')\n                     tar_mask = np.array(tar_mask)\n                     tar_mask = np.where(tar_mask > 128, 1, 0).astype(np.uint8)\n \n+                    # ===== Diversity ========\n+                    \n                     object_mask = extract_foreground_mask(gt_image)\n                     tar_mask = seg_mask_random_placement_region(tar_mask,object_mask)\n \n-                    # ===== Diversity ========\n+                    \n                     #tar_mask = edge_mask_augmentation(tar_mask,object_mask)\n                     #tar_mask = mask_augmentation(tar_mask)\n                     #ref_mask = reference_mask_augmentation(ref_mask)\n                     #ref_image, ref_mask, tar_mask, = rotate_image_and_mask_for_transistor(ref_image, ref_mask, tar_mask)\n"
                },
                {
                    "date": 1754658649500,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -378,11 +378,9 @@\n                     # ===== Diversity ========\n                     \n                     object_mask = extract_foreground_mask(gt_image)\n                     tar_mask = seg_mask_random_placement_region(tar_mask,object_mask)\n-\n-                    \n-                    #tar_mask = edge_mask_augmentation(tar_mask,object_mask)\n+                    #tar_mask = edge_mask_augmentation(tar_mask,object_mask) \n                     #tar_mask = mask_augmentation(tar_mask)\n                     #ref_mask = reference_mask_augmentation(ref_mask)\n                     #ref_image, ref_mask, tar_mask, = rotate_image_and_mask_for_transistor(ref_image, ref_mask, tar_mask)\n                     if save_memory:\n"
                },
                {
                    "date": 1754658703287,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -345,9 +345,9 @@\n     for category in categories:\n         defect_classes = [cls for cls in os.listdir(f\"{dataset_path}/{category}/test\") if cls != \"good\"]\n         for defect_class in defect_classes:\n             print(f\"category:{category}, defect class:{defect_class}\")\n-            save_path = f\"./result/MVTec_few/{category}\"\n+            save_path = f\"./Result/MVTec_few/{category}\"\n             tar_image_paths = glob.glob(os.path.join(dataset_path, category, \"train\", \"good\", \"*.png\")) #MVTec\n             ref_image_paths = glob.glob(os.path.join(dataset_path, category, \"test\", defect_class, \"*.png\")) #MVTec\n             selected_images = select_limited_images(ref_image_paths)\n             for i in range(1):\n"
                },
                {
                    "date": 1754658727333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -339,9 +339,8 @@\n     import os \n \n     dataset_path = \"/auto/master12/xuruyi/github/dataset/MVTec_AD\"\n     \n-    #defect_interest = [\"contamination\",\"deformation\",\"missing_parts\",\"scratch\",\"foreign_objects\"]\n     categories= os.listdir(dataset_path)\n     for category in categories:\n         defect_classes = [cls for cls in os.listdir(f\"{dataset_path}/{category}/test\") if cls != \"good\"]\n         for defect_class in defect_classes:\n"
                }
            ],
            "date": 1754658558284,
            "name": "Commit-0",
            "content": "import cv2\nimport einops\nimport numpy as np\nimport torch\nimport random\nimport gc\nfrom cldm.model import create_model, load_state_dict\nfrom cldm.ddim_hacked import DDIMSampler\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\nfrom datasets.data_utils import * \ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\nimport albumentations as A\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport glob\nfrom atten.shareatten import *\nfrom scipy.ndimage import rotate, zoom, binary_dilation, binary_erosion\nfrom memory_profiler import profile\nfrom skimage.morphology import disk\n\n\n\nsave_memory = False #save memory\n\nconfig = OmegaConf.load('./configs/inference.yaml')\nmodel_ckpt =  config.pretrained_model\nmodel_config = config.config_file\nuse_interactive_seg = config.config_file\n\nmodel = create_model(model_config).cpu()\n\nif save_memory:\n    torch.cuda.set_per_process_memory_fraction(0.6, device=torch.device(\"cuda:0\"))  # Limit usage to 60% of GPU\n    state_dict = load_state_dict(model_ckpt, location='cpu')\n    state_dict = {k: v.to(torch.float16) for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    model = model.cuda()\nelse:\n    model.load_state_dict(load_state_dict(model_ckpt, location='cuda'))\n    model = model.cuda()\n\nddim_sampler = DDIMSampler(model)\n\nif use_interactive_seg:\n    from iseg.coarse_mask_refine_util import BaselineModel\n    model_path = './iseg/coarse_mask_refine.pth'\n    iseg_model = BaselineModel().eval()\n    weights = torch.load(model_path , map_location='cpu')['state_dict']\n    iseg_model.load_state_dict(weights, strict= True)\n\n\ndisable_verbosity()\nif save_memory:\n    print(\"save_memory\")\n    enable_sliced_attention()\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    torch.backends.cudnn.benchmark = True\n\ndef aug_data_mask(image, mask):\n    transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        ])\n    transformed = transform(image=image.astype(np.uint8), mask = mask)\n    transformed_image = transformed[\"image\"]\n    transformed_mask = transformed[\"mask\"]\n    return transformed_image, transformed_mask\n\ndef process_image(image,num_samples):\n    image = torch.from_numpy(image).float().cuda()\n    image = torch.stack([image for _ in range(num_samples)], dim=0)\n    image = einops.rearrange(image, 'b h w c -> b c h w').contiguous()\n    return image\n\ndef process_pairs(ref_image, ref_mask, tar_image, tar_mask, enable_shape_control = False):\n    # ========= Reference ===========\n    # ref expand \n    ref_box_yyxx = get_bbox_from_mask(ref_mask)\n\n    # ref filter mask \n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n    masked_ref_image = ref_image * ref_mask_3 + np.ones_like(ref_image) * 255 * (1-ref_mask_3)\n\n    y1,y2,x1,x2 = ref_box_yyxx\n    masked_ref_image = masked_ref_image[y1:y2,x1:x2,:]\n    ref_mask = ref_mask[y1:y2,x1:x2]\n    ref_background = ref_image[y1:y2,x1:x2]\n    cropped_ref_mask = ref_mask\n\n\n    ratio = np.random.randint(12, 13) / 10\n    masked_ref_image, ref_mask = expand_image_mask(masked_ref_image, ref_mask, ratio=ratio)\n    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n\n    # to square and resize\n    masked_ref_image = pad_to_square(masked_ref_image, pad_value = 255, random = False)\n    masked_ref_image = cv2.resize(masked_ref_image, (224,224) ).astype(np.uint8)\n\n    ref_mask_3 = pad_to_square(ref_mask_3 * 255, pad_value = 0, random = False)\n    ref_mask_3 = cv2.resize(ref_mask_3, (224,224) ).astype(np.uint8)\n    ref_mask = ref_mask_3[:,:,0]\n\n    # ref aug \n    masked_ref_image_aug = masked_ref_image\n\n    # collage aug \n    masked_ref_image_compose, ref_mask_compose = aug_data_mask(masked_ref_image, ref_mask) \n    masked_ref_image_aug = masked_ref_image_compose.copy()\n    ref_mask_3 = np.stack([ref_mask_compose,ref_mask_compose,ref_mask_compose],-1)\n    ref_image_collage = sobel(masked_ref_image_compose, ref_mask_compose/255)\n\n    # ========= Target ===========\n    tar_box_yyxx = get_bbox_from_mask(tar_mask)\n    tar_box_yyxx = expand_bbox(tar_mask, tar_box_yyxx, ratio=[1.1,1.2])\n    tar_box_yyxx_full = tar_box_yyxx\n    # crop\n    tar_box_yyxx_crop =  expand_bbox(tar_image, tar_box_yyxx, ratio=[1.5, 3])    \n    tar_box_yyxx_crop = box2squre(tar_image, tar_box_yyxx_crop) # crop box\n    y1,y2,x1,x2 = tar_box_yyxx_crop\n\n    cropped_target_image = tar_image[y1:y2,x1:x2,:]\n    cropped_tar_mask = tar_mask[y1:y2,x1:x2]\n    tar_box_yyxx = box_in_box(tar_box_yyxx, tar_box_yyxx_crop)\n    y1,y2,x1,x2 = tar_box_yyxx\n\n    # collage\n    ref_image_collage = cv2.resize(ref_image_collage, (x2-x1, y2-y1))\n    ref_mask_compose = cv2.resize(ref_mask_compose.astype(np.uint8), (x2-x1, y2-y1))\n    ref_mask_compose = (ref_mask_compose > 128).astype(np.uint8)\n\n    collage = cropped_target_image.copy() \n    collage[y1:y2,x1:x2,:] = ref_image_collage\n\n    collage_mask = cropped_target_image.copy() * 0.0\n    collage_mask[y1:y2,x1:x2,:] = 1.0\n\n    if enable_shape_control:\n        collage_mask = np.stack([cropped_tar_mask,cropped_tar_mask,cropped_tar_mask],-1)\n\n    # the size before pad\n    H1, W1 = collage.shape[0], collage.shape[1]\n    cropped_target_image = pad_to_square(cropped_target_image, pad_value = 0, random = False).astype(np.uint8)\n    cropped_tar_mask = np.expand_dims(cropped_tar_mask, axis=-1)\n    cropped_tar_mask = pad_to_square(cropped_tar_mask, pad_value = 0, random = False).astype(np.uint8)\n    cropped_ref_mask = np.expand_dims(cropped_ref_mask, axis=-1)\n    cropped_ref_mask = pad_to_square(cropped_ref_mask, pad_value = 0, random = False).astype(np.uint8)\n    collage = pad_to_square(collage, pad_value = 0, random = False).astype(np.uint8)\n    collage_mask = pad_to_square(collage_mask, pad_value = -1, random = False).astype(np.uint8)\n    ref_background = pad_to_square(ref_background, pad_value = 0, random = False).astype(np.uint8)\n    # the size after pad\n    H2, W2 = collage.shape[0], collage.shape[1]\n    cropped_target_image = cv2.resize(cropped_target_image, (512,512)).astype(np.float32)\n    cropped_tar_mask = cv2.resize(cropped_tar_mask.astype(np.uint8), (512,512)).astype(np.float32)\n    ref_background = cv2.resize(ref_background.astype(np.uint8), (512,512)).astype(np.float32)\n    cropped_ref_mask = cv2.resize(cropped_ref_mask.astype(np.uint8), (512,512)).astype(np.float32)\n    collage = cv2.resize(collage, (512,512)).astype(np.float32)\n    collage_mask  = (cv2.resize(collage_mask, (512,512)).astype(np.float32) > 0.5).astype(np.float32)\n\n    masked_ref_image_aug = masked_ref_image_aug  / 255 \n    cropped_target_image = cropped_target_image / 127.5 - 1.0\n    ref_background = ref_background / 127.5 - 1.0\n    collage = collage / 127.5 - 1.0 \n    collage = np.concatenate([collage, collage_mask[:,:,:1]  ] , -1)\n\n    item = dict(ref=masked_ref_image_aug.copy(), \n                jpg=cropped_target_image.copy(), \n                hint=collage.copy(), \n                ref_background=ref_background.copy(),\n                mask=cropped_tar_mask,\n                ref_mask=cropped_ref_mask,\n                extra_sizes=np.array([H1, W1, H2, W2]), \n                tar_box_yyxx_crop=np.array( tar_box_yyxx_crop ),\n                tar_box_yyxx=np.array(tar_box_yyxx_full),\n    )\n    return item\n\ndef crop_back( pred, tar_image,  extra_sizes, tar_box_yyxx_crop):\n    H1, W1, H2, W2 = extra_sizes\n    y1,y2,x1,x2 = tar_box_yyxx_crop    \n    pred = cv2.resize(pred, (W2, H2))\n    m = 3 # maigin_pixel\n\n    if W1 == H1:\n        tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n        return tar_image\n\n    if W1 < W2:\n        pad1 = int((W2 - W1) / 2)\n        pad2 = W2 - W1 - pad1\n        pred = pred[:,pad1: -pad2, :]\n    else:\n        pad1 = int((H2 - H1) / 2)\n        pad2 = H2 - H1 - pad1\n        pred = pred[pad1: -pad2, :, :]\n    tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n    return tar_image\n\n\ndef inference_single_image(ref_image, \n                           ref_mask, \n                           tar_image, \n                           tar_mask):\n    \n    # ==================\n    num_samples = 1 \n    strength = 1  \n    ddim_steps = 50 \n    scale = 3.5   \n    seed = -1  \n    eta = 0.0 \n    start_step = 25\n    guess_mode = False\n    H,W = 512,512\n    model.control_scales = ([strength] * 13)\n    adain_weight = 0.5\n    energy_score = True\n    adaptive_mask = True\n    \n    # ====================\n    raw_background = tar_image.copy()\n    item = process_pairs(ref_image, ref_mask, tar_image, tar_mask, enable_shape_control = True )\n    ref = item['ref']\n    hint = item['hint']\n\n    if save_memory:\n        model.low_vram_shift(is_diffusing=False)\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n        \n    if seed == -1:\n        seed = random.randint(0, 65535)\n    else:\n        seed_everything(seed)\n    \n    control = process_image(hint.copy(),num_samples)\n\n    clip_input = process_image(ref.copy(),num_samples)\n\n    cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning( clip_input )]}\n    un_cond = {\"c_concat\": None if guess_mode else [control], \n               \"c_crossattn\": [model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n    ddim_cond = {\"c_concat\": None, \n               \"c_crossattn\": [model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n    shape = (4, H // 8, W // 8)\n\n    # === Encoder ========\n    target_img = process_image(item['jpg'],num_samples)\n    # target_img = item['jpg']  \n    # target_img = torch.from_numpy(target_img).float().cuda()\n    # target_img = torch.stack([target_img for _ in range(num_samples)], dim=0)\n    # target_img = einops.rearrange(target_img, 'b h w c -> b c h w').clone()\n\n    target_mask = item['mask'] \n    if target_mask is None or not isinstance(target_mask, np.ndarray):\n        raise ValueError(\"target_mask is not a valid numpy array\")\n    target_mask = cv2.resize(target_mask.astype(np.uint8), (64,64)).astype(np.float32)\n    target_mask = torch.from_numpy(target_mask).float().cuda()\n    target_mask = torch.stack([target_mask for _ in range(num_samples)], dim=0)\n    target_mask = einops.rearrange(target_mask, 'b h w -> b 1 h w').clone()\n\n    #if DDIM inversion \n    ref_background = process_image(item['ref_background'],num_samples)\n    \n    ref_ddim_mask = item['ref_mask'] \n    if ref_ddim_mask is None or not isinstance(ref_ddim_mask, np.ndarray):\n        raise ValueError(\"target_mask is not a valid numpy array\")\n    ref_ddim_mask = cv2.resize(ref_ddim_mask.astype(np.uint8), (64,64)).astype(np.float32)\n    ref_ddim_mask = torch.from_numpy(ref_ddim_mask).float().cuda()\n    ref_ddim_mask = torch.stack([ref_ddim_mask for _ in range(num_samples)], dim=0)\n    ref_ddim_mask = einops.rearrange(ref_ddim_mask, 'b h w -> b 1 h w').clone()\n    \n    x0 = model.get_first_stage_encoding(model.encode_first_stage(target_img))\n    x0_tar = model.get_first_stage_encoding(model.encode_first_stage(ref_background))\n    # ==================================\n    if save_memory:\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n        print(\"diffusion to gpu\")\n        model.low_vram_shift(is_diffusing=True)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n\n        ddim_samples, latents_list = ddim_sampler.ddim_inversion(x0_tar, conditioning=ddim_cond, num_steps=ddim_steps)\n        editor = ShareSelfAttentionControl(start_step,10)\n        regiter_attention_editor_ldm(model, editor)\n        ori_ddim_samples, ori_latents_list = ddim_sampler.ddim_inversion(x0, conditioning=ddim_cond, num_steps=ddim_steps)\n        editor.switch_to_use_mode()\n        samples, _ = ddim_sampler.sample(ddim_steps, num_samples,\n                                        shape, cond, verbose=False, eta=0,\n                                        unconditional_guidance_scale=scale,\n                                        unconditional_conditioning=un_cond,\n                                        x0=x0,\n                                        mask=target_mask,\n                                        energy_score=energy_score,\n                                        ref_latents=latents_list,\n                                        ref_mask=ref_ddim_mask,\n                                        tar_latents=ori_latents_list,\n                                        adain_weight=adain_weight,\n                                        x0_tar=x0_tar,\n                                        adaptive_mask=adaptive_mask,\n                                        )\n        editor.reset()    \n\n    if save_memory:\n        print(\"diffusion to cpu\")\n        model.low_vram_shift(is_diffusing=False)\n\n    x_samples = model.decode_first_stage(samples)\n    x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()#.clip(0, 255).astype(np.uint8)\n\n    # result = x_samples[0][:,:,::-1]\n    # result = np.clip(result,0,255)\n\n    pred = x_samples[0]\n    pred = np.clip(pred,0,255)[1:,:,:]\n    sizes = item['extra_sizes']\n    tar_box_yyxx_crop = item['tar_box_yyxx_crop'] \n    tar_image = crop_back(pred, tar_image, sizes, tar_box_yyxx_crop) \n    y1,y2,x1,x2 = item['tar_box_yyxx']\n    raw_background[y1:y2, x1:x2, :] = tar_image[y1:y2, x1:x2, :]\n\n    if save_memory:\n        del ref, hint\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n    return raw_background, item\n\n\n\n\n\ndef select_limited_images(ref_image_paths, fraction=1/3):\n\n    num_images_to_select = max(1, int(len(ref_image_paths) * fraction))\n    \n    selected_images = ref_image_paths[:num_images_to_select]\n    #selected_images = random.sample(ref_image_paths, num_images_to_select)\n    \n    return selected_images\n\nif __name__ == '__main__': \n   \n    from omegaconf import OmegaConf\n    import os \n\n    dataset_path = \"/auto/master12/xuruyi/github/dataset/MVTec_AD\"\n    \n    #defect_interest = [\"contamination\",\"deformation\",\"missing_parts\",\"scratch\",\"foreign_objects\"]\n    categories= os.listdir(dataset_path)\n    for category in categories:\n        defect_classes = [cls for cls in os.listdir(f\"{dataset_path}/{category}/test\") if cls != \"good\"]\n        for defect_class in defect_classes:\n            print(f\"category:{category}, defect class:{defect_class}\")\n            save_path = f\"./result/MVTec_few/{category}\"\n            tar_image_paths = glob.glob(os.path.join(dataset_path, category, \"train\", \"good\", \"*.png\")) #MVTec\n            ref_image_paths = glob.glob(os.path.join(dataset_path, category, \"test\", defect_class, \"*.png\")) #MVTec\n            selected_images = select_limited_images(ref_image_paths)\n            for i in range(1):\n                try:\n                    ref_image_name = random.choice(selected_images)\n                    tar_image_name = random.choice(tar_image_paths)\n                    tar_mask_name = random.choice(selected_images)\n                    ref_mask_path = ref_image_name.replace(\"/test/\", \"/ground_truth/\").replace(\".png\", \"_mask.png\")#mvtec\n                    tar_mask_path = tar_mask_name.replace(\"/test/\", \"/ground_truth/\").replace(\".png\", \"_mask.png\")#mvtec\n                    print(f\"Processing: [{i}] ref_image_name-{ref_image_name}, tar_image_name-{tar_image_name}, tar_mask_name-{tar_mask_name}\")\n                    ref_image = cv2.imread(ref_image_name)\n                    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n\n                    # background image\n                    gt_image = cv2.imread(tar_image_name)\n                    gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n\n                    ref_mask = Image.open(ref_mask_path).convert('L')\n                    ref_mask = np.array(ref_mask)\n                    ref_mask = np.where(ref_mask > 128, 1, 0).astype(np.uint8)\n                    #ref_mask = reference_mask_augmentation(ref_mask)\n\n                    # background mask\n                    tar_mask = Image.open(tar_mask_path).convert('L')\n                    tar_mask = np.array(tar_mask)\n                    tar_mask = np.where(tar_mask > 128, 1, 0).astype(np.uint8)\n\n                    object_mask = extract_foreground_mask(gt_image)\n                    tar_mask = seg_mask_random_placement_region(tar_mask,object_mask)\n\n                    # ===== Diversity ========\n                    #tar_mask = edge_mask_augmentation(tar_mask,object_mask)\n                    #tar_mask = mask_augmentation(tar_mask)\n                    #ref_mask = reference_mask_augmentation(ref_mask)\n                    #ref_image, ref_mask, tar_mask, = rotate_image_and_mask_for_transistor(ref_image, ref_mask, tar_mask)\n                    if save_memory:\n                        torch.cuda.empty_cache()  # Release unused but cached memory\n                        torch.cuda.ipc_collect()  # Force PyTorch to release VRM\n\n                    gen_image, item = inference_single_image(ref_image, ref_mask, gt_image.copy(), tar_mask)\n\n                    synthesis = torch.from_numpy(gen_image).permute(2, 0, 1)\n                    synthesis = synthesis.permute(1, 2, 0).numpy()\n                    ref_processed = item['ref']\n\n                    test_dir = os.path.join(save_path, \"test\", defect_class)\n                    gt_dir = os.path.join(save_path, \"ground_truth\", defect_class)\n                    source_dir = os.path.join(save_path, \"source\", defect_class)\n                    os.makedirs(test_dir, exist_ok=True)\n                    os.makedirs(gt_dir, exist_ok=True)\n                    os.makedirs(source_dir, exist_ok=True)\n\n                    next_index = get_next_image_index(test_dir)\n\n                    # filename\n                    #source_path = os.path.splitext(os.path.basename(tar_image_name))[0]\n                    source_path = source_path.split('_')[1]\n                    image_name = f\"{next_index:04d}_{source_path}.png\"\n                    mask_name = f\"{next_index:04d}_{source_path}_mask.png\"\n\n                    image_path = os.path.join(test_dir, image_name)\n                    mask_path = os.path.join(gt_dir, mask_name)\n                    source_image_path = os.path.join(source_dir, f\"source_{image_name}\")\n\n                    cv2.imwrite(image_path, cv2.cvtColor(synthesis.astype(np.uint8), cv2.COLOR_RGB2BGR))\n                    cv2.imwrite(mask_path, (tar_mask * 255).astype(np.uint8))\n\n                    source_image = compose_images(ref_image, ref_mask, gt_image, tar_mask, synthesis, ref_processed)\n                    cv2.imwrite(source_image_path, cv2.cvtColor(source_image, cv2.COLOR_RGB2BGR))\n\n                    if save_memory:\n                        del ref_image, ref_mask, gt_image, tar_mask, gen_image, item\n                        torch.cuda.empty_cache()\n                        torch.cuda.ipc_collect()\n                        gc.collect()\n                except Exception as e:\n                    print(f\"[Error in iteration {i}] Skipping due to error: {e}\") \n                continue\n         \n\n                \n\n\n\n"
        }
    ]
}